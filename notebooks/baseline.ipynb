{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-28T08:08:36.957083Z",
     "iopub.status.busy": "2022-06-28T08:08:36.956126Z",
     "iopub.status.idle": "2022-06-28T08:08:37.008906Z",
     "shell.execute_reply": "2022-06-28T08:08:37.008115Z",
     "shell.execute_reply.started": "2022-06-28T08:08:36.956962Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T08:08:40.343452Z",
     "iopub.status.busy": "2022-06-28T08:08:40.343009Z",
     "iopub.status.idle": "2022-06-28T08:08:46.914257Z",
     "shell.execute_reply": "2022-06-28T08:08:46.913461Z",
     "shell.execute_reply.started": "2022-06-28T08:08:40.343409Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 tweets loaded\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "labels = []\n",
    "\n",
    "def load_tweets(filename, label):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tweets.append(line.rstrip())\n",
    "            labels.append(label)\n",
    "    \n",
    "load_tweets('../data/train_neg.txt', 0)\n",
    "load_tweets('../data/train_pos.txt', 1)\n",
    "\n",
    "# Convert to NumPy array to facilitate indexing\n",
    "tweets = np.array(tweets)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f'{len(tweets)} tweets loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build validation set\n",
    "We use 90% of tweets for training, and 10% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T08:08:46.917274Z",
     "iopub.status.busy": "2022-06-28T08:08:46.916933Z",
     "iopub.status.idle": "2022-06-28T08:08:46.980123Z",
     "shell.execute_reply": "2022-06-28T08:08:46.979219Z",
     "shell.execute_reply.started": "2022-06-28T08:08:46.917245Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180000, 20000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1) # Reproducibility!\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(tweets))\n",
    "split_idx = int(0.9 * len(tweets))\n",
    "train_indices = shuffled_indices[:split_idx]\n",
    "val_indices = shuffled_indices[split_idx:]\n",
    "\n",
    "len(train_indices), len(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 49673, 171551,   5506, ..., 194791,  79538, 121767])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bag-of-words baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# We only keep the 5000 most frequent words, both to reduce the computational cost and reduce overfitting\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "# Important: we call fit_transform on the training set, and only transform on the validation set\n",
    "X_train = vectorizer.fit_transform(tweets[train_indices])\n",
    "X_val = vectorizer.transform(tweets[val_indices])\n",
    "\n",
    "Y_train = labels[train_indices]\n",
    "Y_val = labels[val_indices]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print(X_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### logistic classifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(C=1e5, max_iter=100)\n",
    "model.fit(X_train, Y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_val_pred = model.predict(X_val)\n",
    "\n",
    "train_accuracy = (Y_train_pred == Y_train).mean()\n",
    "val_accuracy = (Y_val_pred == Y_val).mean()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print(f'Accuracy (training set): {train_accuracy:.05f}')\n",
    "#print(f'Accuracy (validation set): {val_accuracy:.05f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_features = model.coef_[0]\n",
    "sorted_features = np.argsort(model_features)\n",
    "top_neg = sorted_features[:10]\n",
    "top_pos = sorted_features[-10:]\n",
    "\n",
    "mapping = vectorizer.get_feature_names()\n",
    "\n",
    "print('---- Top 10 negative words')\n",
    "for i in top_neg:\n",
    "    print(mapping[i], model_features[i])\n",
    "print()\n",
    "\n",
    "print('---- Top 10 positive words')\n",
    "for i in top_pos:\n",
    "    print(mapping[i], model_features[i])\n",
    "print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CNN baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T08:08:54.218346Z",
     "iopub.status.busy": "2022-06-28T08:08:54.218006Z",
     "iopub.status.idle": "2022-06-28T08:08:59.736466Z",
     "shell.execute_reply": "2022-06-28T08:08:59.735702Z",
     "shell.execute_reply.started": "2022-06-28T08:08:54.218317Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#t = Tokenizer(oov_token='<UNK>')\n",
    "# fit the tokenizer on the documents to create the vocabulary of unique words and map them to row numbers in the embedding layer\n",
    "#t.fit_on_texts(tweets[train_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#t.word_index['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "s = 'the avengers was really great'\n",
    "r = t.texts_to_sequences([s])\n",
    "r\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#train_sequences = t.texts_to_sequences(tweets[train_indices])\n",
    "#val_sequences = t.texts_to_sequences(tweets[val_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
    "#print(\"Number of Documents={}\".format(t.document_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#max([len(sentence_tokens) for sentence_tokens in train_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#max([len(sentence_tokens) for sentence_tokens in val_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_lens = [len(s) for s in train_sequences]\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12, 6))\n",
    "h1 = ax.hist(train_lens)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Sequence Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pad dataset to a maximum review length in words\n",
    "\"\"\"\n",
    "X_train = sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_val = sequence.pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train.shape, X_val.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EMBED_SIZE = 300 # word embedding size\n",
    "EPOCHS=5\n",
    "BATCH_SIZE=16 #128\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VOCAB_SIZE = len(t.word_index)\n",
    "VOCAB_SIZE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "\n",
    "# embedding layer is of dim: 175846 x 300\n",
    "model.add(Embedding(VOCAB_SIZE, \n",
    "                    EMBED_SIZE, \n",
    "                    #weights=[wt_arr],\n",
    "                    input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# callbacks\n",
    "\"\"\"\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                      patience=2,\n",
    "                                      restore_best_weights=True,\n",
    "                                      verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train, \n",
    "          validation_split=0.1,\n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE,\n",
    "          callbacks=[es], \n",
    "          verbose=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T08:09:09.919004Z",
     "iopub.status.busy": "2022-06-28T08:09:09.918389Z",
     "iopub.status.idle": "2022-06-28T08:09:12.111943Z",
     "shell.execute_reply": "2022-06-28T08:09:12.111143Z",
     "shell.execute_reply.started": "2022-06-28T08:09:09.918963Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import transformers\n",
    "import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BERT Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_bert_input_features(tokenizer, docs, max_seq_length):\n",
    "    \n",
    "    all_ids, all_masks = [], []\n",
    "    for doc in tqdm.tqdm(docs, desc=\"Converting docs to features\"):\n",
    "        \n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        \n",
    "        if len(tokens) > max_seq_length-2:\n",
    "            tokens = tokens[0 : (max_seq_length-2)]\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        masks = [1] * len(ids)\n",
    "        \n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(ids) < max_seq_length:\n",
    "            ids.append(0)\n",
    "            masks.append(0)\n",
    "            \n",
    "        all_ids.append(ids)\n",
    "        all_masks.append(masks)\n",
    "        \n",
    "    encoded = np.array([all_ids, all_masks])\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T08:09:41.923624Z",
     "iopub.status.busy": "2022-06-28T08:09:41.923058Z",
     "iopub.status.idle": "2022-06-28T08:10:06.181209Z",
     "shell.execute_reply": "2022-06-28T08:10:06.180264Z",
     "shell.execute_reply.started": "2022-06-28T08:09:41.923587Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 70\n",
    "\n",
    "inp_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_ids\")\n",
    "inp_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_masks\")\n",
    "inputs = [inp_id, inp_mask]\n",
    "\n",
    "hidden_state = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')(inputs)[0]\n",
    "pooled_output = hidden_state[:, 0]    \n",
    "\n",
    "dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled_output)\n",
    "drop1 = tf.keras.layers.Dropout(0.25)(dense1)\n",
    "dense2 = tf.keras.layers.Dense(256, activation='relu')(drop1)\n",
    "drop2 = tf.keras.layers.Dropout(0.25)(dense2)\n",
    "\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(drop2)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=2e-5, \n",
    "                                           epsilon=1e-08), \n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convert text to DistilBERT input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_features_ids, train_features_masks = create_bert_input_features(tokenizer, tweets[train_indices], \n",
    "                                                                      max_seq_length=MAX_SEQ_LENGTH)\n",
    "val_features_ids, val_features_masks = create_bert_input_features(tokenizer, tweets[val_indices], \n",
    "                                                                  max_seq_length=MAX_SEQ_LENGTH)\n",
    "#test_features = create_bert_input_features(tokenizer, test_reviews, max_seq_length=MAX_SEQ_LENGTH)\n",
    "print('Train Features:', train_features_ids.shape, train_features_masks.shape)\n",
    "print('Val Features:', val_features_ids.shape, val_features_masks.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y_train = labels[train_indices]\n",
    "Y_val = labels[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y_train = labels[train_indices]\n",
    "Y_val = labels[val_indices]\n",
    "\n",
    "val_features_ids, val_features_masks = create_bert_input_features(tokenizer, tweets[val_indices], \n",
    "                                                                  max_seq_length=MAX_SEQ_LENGTH)\n",
    "                                                                  \n",
    "                                                                  \n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                      patience=1,\n",
    "                                      restore_best_weights=True,\n",
    "                                      verbose=1)                                                            \n",
    "\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_features_ids, train_features_masks = create_bert_input_features(tokenizer, tweets[train_indices][:1000],    #1125001\n",
    "                                                                      max_seq_length=MAX_SEQ_LENGTH)  \n",
    "\n",
    "model.fit([train_features_ids, \n",
    "           train_features_masks], Y_train[:1000], \n",
    "          validation_data=([val_features_ids, \n",
    "                            val_features_masks], Y_val),\n",
    "          epochs=2, \n",
    "          batch_size=20, \n",
    "          shuffle=True,\n",
    "          callbacks=[es],\n",
    "          verbose=1)\n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#del train_features_ids, train_features_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_features_ids, train_features_masks = create_bert_input_features(tokenizer, tweets[train_indices][1125001:], \n",
    "                                                                      max_seq_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "model.fit([train_features_ids, \n",
    "           train_features_masks], Y_train[1125001:], \n",
    "          validation_data=([val_features_ids, \n",
    "                            val_features_masks], Y_val),\n",
    "          epochs=3, \n",
    "          batch_size=20, \n",
    "          shuffle=True,\n",
    "          callbacks=[es],\n",
    "          verbose=1)\n",
    "\"\"\"\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train and Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                      patience=1,\n",
    "                                      restore_best_weights=True,\n",
    "                                      verbose=1)\n",
    "model.fit([train_features_ids, \n",
    "           train_features_masks], train_sentiments, \n",
    "          validation_data=([val_features_ids, \n",
    "                            val_features_masks], val_sentiments),\n",
    "          epochs=3, \n",
    "          batch_size=20, \n",
    "          shuffle=True,\n",
    "          callbacks=[es],\n",
    "          verbose=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T08:12:55.175526Z",
     "iopub.status.busy": "2022-06-28T08:12:55.174692Z",
     "iopub.status.idle": "2022-06-28T08:12:55.24516Z",
     "shell.execute_reply": "2022-06-28T08:12:55.243409Z",
     "shell.execute_reply.started": "2022-06-28T08:12:55.175489Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "modelname = 'model_BERT_v1\"\n",
    "tf.keras.models.save_model(model, './export/' + modelname + '/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('gpflow-3.9.0': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a340a5e3febb3027518163e758cac03f052ae3a2e42684e4a985ce13300c1d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}