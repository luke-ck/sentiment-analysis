from bunch import Bunch
import pathlib
from os import path
import os
import yaml
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
# Weights&Biases logging
import torch
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint


project_root = pathlib.Path(__file__).parent.parent.absolute()
config_path = path.join(project_root, "config", "config.yaml")
normalized_log = []


def read_args() -> Bunch:
    with open(config_path, "r") as config_file:
        config_dict = yaml.safe_load(config_file)
    return Bunch(config_dict)


def build_paths(save_path: str) -> None:
    checkpoint_path = save_path + '/checkpoints'
    if path.isdir(save_path) is False and save_path is not None:
        os.mkdir(save_path)

    if path.isdir(checkpoint_path) is False:
        os.mkdir(checkpoint_path)


def compute_class_probabilities(pc, model_probs, prior_probs):
    """
    Computes the probability of each sample belonging to each class given its predicted probability distribution.

    Args:
    - pc(batch_size, num_classes): contains the predicted probability distribution for each sample.
    - model_probs(num_classes, num_models): contains the probability distributions generated by each model for each class.
    - prior_probs(num_classes,): contains the prior probabilities for each class.

    Returns:
    - class_probs: A tensor of shape (batch_size, num_classes) containing the probability of each sample belonging to each class.
    """

    marginal_prob = torch.sum(pc * model_probs, dim=1)

    # Compute the probability of each class given the predicted probability distribution using Bayes' theorem
    class_probs = torch.zeros_like(pc)
    for i in range(pc.shape[1]):
        class_probs[:, i] = (pc[:, i] * model_probs[i] * prior_probs[i]) / marginal_prob

    return class_probs

def initialize_trainer(save_path: str, config: Bunch) -> Trainer:
    entity = config.wandb["entity"]
    project = config.wandb["project"]
    api_key = config.wandb["api_key"]
    patience = config.model['patience']
    min_delta = config.model['min_delta']
    max_epochs = config.model["epochs"]
    seed = config.preprocessing['seed']
    fast_dev_run = config.trainer['fast_dev_run']
    log_every_n_steps = config.trainer['log_freq']
    prefix = config.trainer['checkpoint_model_format']
    val_check_interval = config.trainer['val_check_interval']
    gradient_clip_val = config.trainer['gradient_clip_val']
    limit_train_batches = config.trainer['limit_train_batches']

    if config.trainer['accelerator'] is not None:
        accelerator = config.trainer['accelerator']
    else:
        accelerator = 'auto'

    if config.trainer['use_amp']:
        precision = "16-mixed"
        torch.set_float32_matmul_precision('high')
    else:
        precision = 32

    number_of_gpus = 1 if torch.cuda.is_available() else 0
    checkpoint_path = save_path + '/checkpoints'
    build_paths(save_path)
    seed_everything(seed, workers=True)
    os.environ["TOKENIZERS_PARALLELISM"] = "true"
    # <----------------WANDB LOGGING-------------------------------->
    try:
        os.environ["WANDB_API_KEY"] = api_key
    except TypeError:  # this will break if the api_key is NoneType
        print("API key has not been set for logging... wandb will be run offline")
        os.environ['WANDB_MODE'] = "offline"

    wandb_logger = WandbLogger(
        entity=entity,
        project=project,
        dir=save_path
    )
    # <-----------------CALLBACKS------------------------------------->
    early_stopping_callback = EarlyStopping(monitor='val_loss',
                                            patience=patience,
                                            min_delta=min_delta,
                                            verbose=True)

    checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_path,
                                          save_top_k=2,
                                          monitor="val_loss",
                                          filename=prefix + '-{epoch:02d}-{val_loss:.2f}')

    return Trainer(
        precision=precision,
        accelerator=accelerator,
        deterministic=False,
        val_check_interval=val_check_interval,
        limit_train_batches=limit_train_batches,
        gradient_clip_val=gradient_clip_val,
        fast_dev_run=fast_dev_run,
        devices=number_of_gpus,
        callbacks=[early_stopping_callback, checkpoint_callback],
        max_epochs=max_epochs,
        logger=wandb_logger,  # W&B integration
        log_every_n_steps=log_every_n_steps,
    )

def create_ensemble_classifiers(random_forest=True, xgboost=True):
    """
    Creates the ensemble classifiers to be used for the ensemble model.

    Args:
    - random_forest: A boolean indicating whether to use a random forest classifier.
    - xgboost: A boolean indicating whether to use an XGBoost classifier.

    Returns: A list of classifiers and a list of their names.

    """
    classifier_list = []
    classifier_names = []
    if random_forest:
        classifier_list.append(RandomForestClassifier(n_estimators=100))
        classifier_names.append("random_forest")
    if xgboost:
        classifier_list.append(xgb.XGBClassifier(n_estimators=100, learning_rate=0.1))
        classifier_names.append("xgboost")

    return classifier_list, classifier_names