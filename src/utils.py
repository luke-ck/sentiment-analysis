import json
from bunch import Bunch
import pathlib
from os import path
import os
import yaml
# Weights&Biases logging
import torch
import numpy as np
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint
from tqdm import tqdm

project_root = pathlib.Path(__file__).parent.parent.absolute()
config_path = path.join(project_root, "config", "config.yaml")
normalized_log = []


def read_args() -> Bunch:
    with open(config_path, "r") as config_file:
        config_dict = yaml.safe_load(config_file)
    return Bunch(config_dict)


def build_paths(save_path: str) -> None:
    checkpoint_path = save_path + '/checkpoints'
    if path.isdir(save_path) is False and save_path is not None:
        os.mkdir(save_path)

    if path.isdir(checkpoint_path) is False:
        os.mkdir(checkpoint_path)


def compute_class_probabilities(pc, model_probs, prior_probs):
    """
    Computes the probability of each sample belonging to each class given its predicted probability distribution.

    Args:
    - pc(batch_size, num_classes): contains the predicted probability distribution for each sample.
    - model_probs(num_classes, num_models): contains the probability distributions generated by each model for each class.
    - prior_probs(num_classes,): contains the prior probabilities for each class.

    Returns:
    - class_probs: A tensor of shape (batch_size, num_classes) containing the probability of each sample belonging to each class.
    """

    marginal_prob = torch.sum(pc * model_probs, dim=1)

    # Compute the probability of each class given the predicted probability distribution using Bayes' theorem
    class_probs = torch.zeros_like(pc)
    for i in range(pc.shape[1]):
        class_probs[:, i] = (pc[:, i] * model_probs[i] * prior_probs[i]) / marginal_prob

    return class_probs


import torch
import pytorch_lightning as pl


class InferenceModule(pl.LightningModule):
    def __init__(self, models, trainer):
        super().__init__()
        self.models = models
        self._trainer = trainer

    def forward(self, x, y):
        results = torch.zeros((x.size(0), len(self.models)))
        for i, model in enumerate(self.models):
            results[:, i] = torch.sigmoid(model(x, y)).squeeze(1)
        return results

    def inference_loop(self, dataloader):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        num_models = len(self.models)
        num_samples = len(dataloader.dataset)
        predictions = torch.zeros((num_samples, num_models))
        labels = torch.zeros(num_samples)

        # Set all models to evaluation mode
        for model in self.models:
            model.eval()

        # Loop over the data loader and compute predictions for each model
        with torch.no_grad(), tqdm(total=len(dataloader)) as progress_bar:
            for i, (inputs, mask, target) in enumerate(dataloader):
                inputs = inputs.to(device)
                mask = mask.to(device)
                batch_size = inputs.size(0)

                outputs = self(inputs, mask)
                for j in range(num_models):
                    predictions[i * batch_size:(i + 1) * batch_size, j] = outputs[:, j].cpu()

                labels[i * batch_size:(i + 1) * batch_size] = target.view(-1).cpu()
                progress_bar.update(1)

        return predictions, labels


def combine_predictions(models, dataloader, logger: WandbLogger = None):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    num_models = len(models)
    num_samples = len(dataloader.dataset)
    predictions = torch.zeros((num_samples, num_models))
    progress_bar = logger.ProgressBar(total=num_samples)
    for model in models:
        model.eval()

    for inputs, mask, target in dataloader:
        preds = torch.stack([
            torch.sigmoid(model(inputs.to("cuda:0"), mask.to("cuda:0"))).cpu().numpy()
            for model in models
        ])

    return np.array(combined_preds), np.array(labels)


def ensemble_predict(models, ensemble_model, dataloader):
    combined_preds, _ = combine_predictions(models, dataloader)
    ensemble_preds = ensemble_model.predict(combined_preds)
    return ensemble_preds


def initialize_trainer(save_path: str, config: Bunch) -> Trainer:
    entity = config.wandb["entity"]
    project = config.wandb["project"]
    api_key = config.wandb["api_key"]
    patience = config.model['patience']
    min_delta = config.model['min_delta']
    max_epochs = config.model["epochs"]
    seed = config.preprocessing['seed']
    fast_dev_run = config.trainer['fast_dev_run']
    log_every_n_steps = config.trainer['log_freq']
    prefix = config.trainer['checkpoint_model_format']
    val_check_interval = config.trainer['val_check_interval']
    gradient_clip_val = config.trainer['gradient_clip_val']
    limit_train_batches = config.trainer['limit_train_batches']

    if config.trainer['accelerator'] is not None:
        accelerator = config.trainer['accelerator']
    else:
        accelerator = 'auto'

    if config.trainer['use_amp']:
        precision = "16-mixed"
        torch.set_float32_matmul_precision('high')
    else:
        precision = 32

    number_of_gpus = 1 if torch.cuda.is_available() else 0
    checkpoint_path = save_path + '/checkpoints'
    build_paths(save_path)
    seed_everything(seed, workers=True)
    os.environ["TOKENIZERS_PARALLELISM"] = "true"
    # <----------------WANDB LOGGING-------------------------------->
    try:
        os.environ["WANDB_API_KEY"] = api_key
    except TypeError:  # this will break if the api_key is NoneType
        print("API key has not been set for logging... wandb will be run offline")
        os.environ['WANDB_MODE'] = "offline"

    wandb_logger = WandbLogger(
        entity=entity,
        project=project,
        dir=save_path
    )
    # <-----------------CALLBACKS------------------------------------->
    early_stopping_callback = EarlyStopping(monitor='val_loss',
                                            patience=patience,
                                            min_delta=min_delta,
                                            verbose=True)

    checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_path,
                                          save_top_k=2,
                                          monitor="val_loss",
                                          filename=prefix + '-{epoch:02d}-{val_loss:.2f}')

    return Trainer(
        precision=precision,
        accelerator=accelerator,
        deterministic=False,
        val_check_interval=val_check_interval,
        limit_train_batches=limit_train_batches,
        gradient_clip_val=gradient_clip_val,
        fast_dev_run=fast_dev_run,
        devices=number_of_gpus,
        callbacks=[early_stopping_callback, checkpoint_callback],
        max_epochs=max_epochs,
        logger=wandb_logger,  # W&B integration
        log_every_n_steps=log_every_n_steps,
    )
