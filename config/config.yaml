preprocessing:
  data_path: null # ../data
  train_neg_small_file_name: null
  train_pos_small_file_name: null
  train_neg_full_file_name: null
  train_pos_full_file_name: null
  test_data: null
  use_full_train_data: true
  seed: 42
  validation_size: 0.1
  train_data_cache_path: null
  val_data_cache_path: null
  test_data_cache_path: null
  symspell_max_dictionary_edit_distance: 3
  symspell_letters_per_typo: 2
  clean_data_leakage_before_normalization: false
  clean_data_leakage_after_normalization: false
  # turn these off and on to control normalisation.
  uncensor_swearwords: true
  check_spelling: false
  # note that tokenizing emojis can only be done in conjunction with normalization. The converse is not true.
  translate_emoticons_to_emoji: false
  normalize: true
  # this removes data leakage with respect to paranthesis rules
  clean_data_leakage: false
  gibberish_detector_limit: 3.0
active_learning:
  predict_active_learning_logits: true
meta_learning:
  paths: [] # specify meta learning model paths here
  save_train_data: True # False loads the saved train data
  save_val_data: True # False loads the saved val data
  random_forest: True
  xgboost: True
model:
  mode: "training"
  # 128 is maximum length admissible by bertweet
  max_token_length: 128
  pretrained_model: vinai/bertweet-base
  batch_size: 2048
  num_workers: 8
  epochs: 20
  learning_rate: 2e-5
  # scheduler stuff
  min_delta: 0.01
  patience: 3
# WandB related settings
wandb:
  entity: null
  # Change The following line to something more descriptive when doing proper runs
  project: null
  save_path: null
  api_key: null
trainer:
  # this should just be FILENAME. checkpoints are saved under save_path/checkpoints
  checkpoint_model: null
  # checkpoint model format. we add -{epoch:02d}-{val_loss:.2f} to this
  checkpoint_model_format: null
  accelerator: gpu
  # every half of a training epoch do some evaluation
  val_check_interval: 1.0
  # for the parameters below, mains accept only value depending on parameter
  # clip gradients, so we don't get weird spikes in loss due to batch size being too large
  gradient_clip_val: 1.0
  # how much of the training data do you want to use
  limit_train_batches: 0.3
  # nvidia apex stuff. If true, set optimization level to O2, and backend to apex
  use_amp: true
  # whether you're debugging or not for a fast run
  fast_dev_run: false
  # logging frequency in steps
  log_freq: 100
